# Base configuration template for RL algorithms

# Algorithm-specific settings  
algorithm:
  name: "base_rl"
  type: "value_based"  # "value_based", "policy_based", "actor_critic"

# Training hyperparameters
training:
  num_episodes: 1000
  max_steps_per_episode: 1000
  batch_size: 32
  learning_rate: 1e-4
  gamma: 0.99  # Discount factor
  
  # Exploration settings
  exploration:
    type: "epsilon_greedy"  # "epsilon_greedy", "boltzmann", "ucb"
    epsilon_start: 1.0
    epsilon_end: 0.01
    epsilon_decay: 0.995
    temperature: 1.0  # For Boltzmann exploration
    
  # Experience replay (for off-policy algorithms)
  replay_buffer:
    enabled: true
    capacity: 100000
    min_size: 1000
    
  # Target network updates (for DQN-style algorithms)
  target_network:
    enabled: true
    update_frequency: 100  # Update every N episodes
    tau: 0.005  # Soft update parameter
    
  # Optimizer settings
  optimizer:
    type: "adam"
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 0.0

# Model architecture
model:
  type: "dqn"  # "dqn", "policy_net", "actor_critic"
  hidden_dims: [256, 256]
  activation: "relu"
  dropout: 0.0
  dueling: false  # For DQN variants

# Environment settings
environment:
  name: "CartPole-v1"
  max_episode_steps: null
  render_mode: null
  reward_scaling: 1.0
  
# Evaluation settings
evaluation:
  enabled: true
  num_episodes: 100
  eval_interval: 100  # Evaluate every N training episodes
  render: false
  deterministic: true  # Use deterministic policy for evaluation

# Logging and monitoring
logging:
  log_interval: 100
  save_interval: 500
  video_interval: 1000  # Record video every N episodes
  
# Checkpointing
checkpointing:
  enabled: true
  save_best_only: false
  monitor: "avg_reward"
  mode: "max"
  save_top_k: 5