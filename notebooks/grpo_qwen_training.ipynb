{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GRPO Training with Qwen 0.5B for Mathematical Reasoning\n\nThis notebook demonstrates training a small Qwen model using Group Relative Policy Optimization (GRPO) for improving mathematical problem-solving capabilities.\n\n## Features:\n- Downloads and loads Qwen 0.5B model\n- Uses GSM8K math dataset for training\n- Custom prompt formatting for math problems\n- Math-specific reward function\n- Tracks training metrics\n- Saves checkpoints"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_needed(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Ensure simple_rl is installed\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"..\"])\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import GRPO from simple_rl\n",
    "from simple_rl.algorithms.grpo import GRPO\n",
    "from simple_rl.utils.config import Config\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Qwen 0.5B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"  # Small 0.5B parameter model\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load GSM8K math dataset\nprint(\"Loading GSM8K math dataset...\")\n\n# Load GSM8K - a dataset of grade school math problems\ndataset = load_dataset(\"gsm8k\", \"main\", split=\"train[:200]\")  # Use 200 examples for demo\n\n# Extract math problems and prepare prompts\nmath_prompts = []\nmath_answers = []  # Store answers for reward calculation\n\nfor item in dataset:\n    # GSM8K format: question and answer with step-by-step solution\n    question = item['question']\n    answer = item['answer']\n    \n    # Store just the question as prompt (without formatting yet)\n    math_prompts.append(question)\n    \n    # Extract the final numerical answer (after ####)\n    final_answer = answer.split(\"####\")[-1].strip() if \"####\" in answer else answer\n    math_answers.append(final_answer)\n\nprint(f\"Loaded {len(math_prompts)} math problems\")\nprint(\"\\nExample problems:\")\nfor i in range(min(3, len(math_prompts))):\n    print(f\"\\n{i+1}. Question: {math_prompts[i][:150]}...\")\n    print(f\"   Answer: {math_answers[i]}\")\n\n# Store answers for later use in reward function\nPROBLEM_ANSWERS = dict(zip(math_prompts, math_answers))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure GRPO Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GRPO Configuration with math-specific prompt formatting\ngrpo_config = {\n    \"algorithm\": {\n        \"name\": \"grpo\",\n        \"group_size\": 4,  # Number of completions per prompt\n        \"kl_coef\": 0.05,  # KL divergence coefficient\n        \"clip_range\": 0.2,  # PPO-style clipping\n        \"normalize_rewards\": True,  # Normalize rewards per group\n        \"update_epochs\": 2,  # PPO update epochs\n        \"minibatch_size\": 4,  # Minibatch size for updates\n    },\n    \"training\": {\n        \"batch_size\": 8,  # Total batch size (must be divisible by group_size)\n        \"learning_rate\": 1e-5,  # Learning rate\n        \"gradient_clip\": 1.0,  # Gradient clipping\n        \"max_new_tokens\": 150,  # More tokens for math reasoning\n        \"temperature\": 0.7,  # Generation temperature\n        \"num_episodes\": 30,  # More episodes for math training\n    },\n    \"generation\": {\n        # Math-specific prompt formatting\n        \"system_prompt\": \"You are a helpful math tutor. Solve the problem step by step, showing your work clearly.\",\n        \"prompt_template\": \"Problem: {prompt}\\n\\nSolution: Let me solve this step by step.\\n\",\n        \"response_prefix\": \"\",\n    },\n    \"model\": {\n        \"max_length\": 512,  # Maximum sequence length\n    },\n    \"logging\": {\n        \"log_interval\": 2,  # Log every 2 episodes\n        \"save_interval\": 10,  # Save checkpoint every 10 episodes\n    },\n    \"wandb\": {\n        \"enabled\": False,  # Disable W&B for notebook demo\n    }\n}\n\n# Create config object\nconfig = Config(**grpo_config)\n\nprint(\"GRPO Configuration for Math Training:\")\nprint(json.dumps(grpo_config, indent=2))\n\n# Show example of formatted prompt\nexample_prompt = math_prompts[0] if math_prompts else \"What is 2 + 2?\"\nprint(f\"\\nExample formatted prompt:\")\nprint(\"-\" * 50)\nformatted_example = grpo_config[\"generation\"][\"system_prompt\"] + \"\\n\\n\" + \\\n                   grpo_config[\"generation\"][\"prompt_template\"].replace(\"{prompt}\", example_prompt)\nprint(formatted_example)\nprint(\"-\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\n\ndef extract_number(text):\n    \"\"\"Extract the final numerical answer from text.\"\"\"\n    # Look for patterns like \"answer is X\", \"equals X\", \"= X\", etc.\n    patterns = [\n        r\"answer is:?\\s*([-]?\\d+\\.?\\d*)\",\n        r\"equals?\\s*([-]?\\d+\\.?\\d*)\",\n        r\"=\\s*([-]?\\d+\\.?\\d*)\",\n        r\"total of\\s*([-]?\\d+\\.?\\d*)\",\n        r\"result is:?\\s*([-]?\\d+\\.?\\d*)\",\n        r\"Therefore,?\\s*([-]?\\d+\\.?\\d*)\",\n        r\"So,?\\s*([-]?\\d+\\.?\\d*)\",\n        r\"\\$?([-]?\\d+\\.?\\d*)\\s*(?:dollars?|cents?|items?|students?|people|apples?|cookies?)?\\.?\\s*$\"\n    ]\n    \n    text_lower = text.lower()\n    for pattern in patterns:\n        match = re.search(pattern, text_lower)\n        if match:\n            return match.group(1)\n    \n    # If no pattern matches, look for any number at the end\n    numbers = re.findall(r\"[-]?\\d+\\.?\\d*\", text)\n    if numbers:\n        return numbers[-1]\n    return None\n\ndef compute_math_reward(prompt: str, completion: str) -> float:\n    \"\"\"\n    Reward function for mathematical problem solving.\n    \n    Rewards based on:\n    - Correct final answer (most important)\n    - Showing work/steps\n    - Mathematical reasoning indicators\n    - Proper formatting\n    \"\"\"\n    reward = 0.0\n    \n    # Check if we have the correct answer for this problem\n    correct_answer = PROBLEM_ANSWERS.get(prompt, None)\n    \n    if correct_answer:\n        # Extract the model's answer\n        model_answer = extract_number(completion)\n        \n        if model_answer:\n            try:\n                # Check if the answer is correct (allowing for small float differences)\n                correct_float = float(correct_answer)\n                model_float = float(model_answer)\n                \n                if abs(correct_float - model_float) < 0.01:\n                    reward += 2.0  # Big reward for correct answer\n                else:\n                    # Partial credit for being close\n                    if abs(correct_float - model_float) / max(abs(correct_float), 1) < 0.1:\n                        reward += 0.5\n                    else:\n                        reward -= 0.5  # Penalty for wrong answer\n            except ValueError:\n                # If conversion fails, do string comparison\n                if model_answer == correct_answer:\n                    reward += 2.0\n                else:\n                    reward -= 0.3\n        else:\n            # No answer found\n            reward -= 1.0\n    \n    # Reward for showing mathematical work\n    math_indicators = [\n        \"step\", \"first\", \"then\", \"next\", \"finally\",\n        \"calculate\", \"multiply\", \"divide\", \"add\", \"subtract\",\n        \"=\", \"+\", \"-\", \"*\", \"/\", \"×\", \"÷\"\n    ]\n    \n    work_shown = sum(1 for indicator in math_indicators if indicator in completion.lower())\n    if work_shown >= 3:\n        reward += 0.5  # Reward for showing work\n    elif work_shown == 0:\n        reward -= 0.3  # Penalty for no work shown\n    \n    # Reward for reasonable length (not too short, not too long)\n    completion_length = len(completion.split())\n    if 20 <= completion_length <= 200:\n        reward += 0.2\n    elif completion_length < 10:\n        reward -= 0.5  # Too short\n    elif completion_length > 300:\n        reward -= 0.2  # Too verbose\n    \n    # Penalty for repetition\n    sentences = completion.split('.')\n    if len(sentences) > 1:\n        unique_sentences = len(set(sentences))\n        if unique_sentences / len(sentences) < 0.7:\n            reward -= 0.5  # Repetitive\n    \n    # Penalty for obvious errors or nonsense\n    if \"error\" in completion.lower() or \"sorry\" in completion.lower():\n        reward -= 0.5\n    \n    return reward\n\n# Test the reward function\ntest_cases = [\n    (\"What is 5 + 3?\", \"5 + 3 = 8. The answer is 8.\"),\n    (\"What is 5 + 3?\", \"Let me calculate: 5 + 3 equals 7.\"),\n    (\"What is 5 + 3?\", \"8\"),\n    (\"What is 5 + 3?\", \"First, I'll add 5 and 3. 5 + 3 = 8. Therefore, the answer is 8.\"),\n]\n\n# Set up test answer\nPROBLEM_ANSWERS[\"What is 5 + 3?\"] = \"8\"\n\nprint(\"Testing math reward function:\")\nfor prompt, completion in test_cases:\n    reward = compute_math_reward(prompt, completion)\n    print(f\"\\nPrompt: {prompt}\")\n    print(f\"Completion: {completion}\")\n    print(f\"Reward: {reward:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize GRPO with Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize GRPO with the loaded model and math reward function\nprint(\"Initializing GRPO with math-specific configuration...\")\n\n# Create GRPO instance with math reward\ngrpo = GRPO(\n    model=model,\n    config=config.to_dict(),\n    tokenizer=tokenizer,\n    reward_fn=compute_math_reward,  # Using our math-specific reward function\n    use_wandb=False\n)\n\nprint(\"GRPO initialized successfully!\")\nprint(f\"Policy model parameters: {sum(p.numel() for p in grpo.model.parameters()) / 1e6:.1f}M\")\nprint(f\"Reference model parameters: {sum(p.numel() for p in grpo.reference_model.parameters()) / 1e6:.1f}M\")\n\n# Show how prompts will be formatted\nprint(\"\\nPrompt formatting example:\")\nsample_problem = \"If John has 5 apples and Mary gives him 3 more, how many apples does John have?\"\nformatted = grpo.format_prompt(sample_problem)\nprint(\"Original:\", sample_problem)\nprint(\"Formatted:\", formatted)"
  },
  {
   "cell_type": "markdown",
   "source": "## 7.5 Experiment with Different Math Prompt Formats\n\nYou can dynamically change how math problems are presented to the model:",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Experiment with different prompt formats for math problems\ntest_problem = \"A store sells pencils for $0.50 each. If Sarah buys 8 pencils, how much does she pay?\"\n\nprint(\"Testing different prompt formats for math problems:\\n\")\n\n# Format 1: Step-by-step instruction\nprint(\"1. Step-by-step format:\")\ngrpo.set_generation_prompt(\n    system_prompt=\"Solve this math problem step by step. Show all your work.\",\n    prompt_template=\"Question: {prompt}\\n\\nStep-by-step solution:\",\n    response_prefix=\"\\n\"\n)\nprint(grpo.format_prompt(test_problem))\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Format 2: Chain-of-thought prompting\nprint(\"2. Chain-of-thought format:\")\ngrpo.set_generation_prompt(\n    system_prompt=\"Think through this problem carefully.\",\n    prompt_template=\"{prompt}\\n\\nLet's think step by step:\",\n    response_prefix=\" \"\n)\nprint(grpo.format_prompt(test_problem))\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Format 3: Structured math format\nprint(\"3. Structured format:\")\ngrpo.set_generation_prompt(\n    system_prompt=None,  # No system prompt\n    prompt_template=\"Math Problem:\\n{prompt}\\n\\nGiven information:\\n- \\n\\nCalculation:\\n\",\n    response_prefix=\"\"\n)\nprint(grpo.format_prompt(test_problem))\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Reset to original configuration for training\nprint(\"Resetting to training configuration...\")\ngrpo.set_generation_prompt(\n    system_prompt=grpo_config[\"generation\"][\"system_prompt\"],\n    prompt_template=grpo_config[\"generation\"][\"prompt_template\"],\n    response_prefix=grpo_config[\"generation\"][\"response_prefix\"]\n)\nprint(\"Ready for training!\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop with Metrics Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training metrics storage\ntraining_metrics = {\n    \"episode\": [],\n    \"total_loss\": [],\n    \"pg_loss\": [],\n    \"kl_div\": [],\n    \"mean_reward\": [],\n    \"reward_std\": [],\n}\n\n# Create checkpoint directory\ncheckpoint_dir = Path(\"checkpoints/grpo_qwen_math\")\ncheckpoint_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Starting GRPO training for {grpo_config['training']['num_episodes']} episodes...\")\nprint(\"Training on math problems from GSM8K dataset\")\nprint(\"=\" * 50)\n\n# Training loop\nfor episode in range(grpo_config['training']['num_episodes']):\n    # Sample batch of math problems\n    batch_size = grpo_config['training']['batch_size'] // grpo_config['algorithm']['group_size']\n    batch_indices = np.random.choice(len(math_prompts), batch_size, replace=True)\n    batch_prompts = [math_prompts[i] for i in batch_indices]\n    \n    # Prepare batch data (prompts will be formatted internally by GRPO)\n    batch_data = {\"prompts\": batch_prompts}\n    \n    # Training step\n    metrics = grpo.train_step(batch_data)\n    \n    # Store metrics\n    training_metrics[\"episode\"].append(episode)\n    training_metrics[\"total_loss\"].append(metrics[\"total_loss\"])\n    training_metrics[\"pg_loss\"].append(metrics[\"pg_loss\"])\n    training_metrics[\"kl_div\"].append(metrics[\"kl_div\"])\n    training_metrics[\"mean_reward\"].append(metrics[\"mean_reward\"])\n    training_metrics[\"reward_std\"].append(metrics.get(\"reward_std\", 0.0))\n    \n    # Logging\n    if episode % grpo_config['logging']['log_interval'] == 0:\n        print(f\"Episode {episode:3d} | \"\n              f\"Loss: {metrics['total_loss']:7.4f} | \"\n              f\"PG Loss: {metrics['pg_loss']:7.4f} | \"\n              f\"KL: {metrics['kl_div']:7.4f} | \"\n              f\"Reward: {metrics['mean_reward']:6.3f} ± {metrics.get('reward_std', 0.0):5.3f}\")\n    \n    # Save checkpoint\n    if (episode + 1) % grpo_config['logging']['save_interval'] == 0:\n        checkpoint_path = checkpoint_dir / f\"checkpoint_episode_{episode+1}.pt\"\n        grpo.save_checkpoint(str(checkpoint_path))\n        print(f\"  → Saved checkpoint to {checkpoint_path}\")\n\nprint(\"=\" * 50)\nprint(\"Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('GRPO Training Metrics', fontsize=16)\n",
    "\n",
    "# Total Loss\n",
    "axes[0, 0].plot(training_metrics[\"episode\"], training_metrics[\"total_loss\"], 'b-', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Loss')\n",
    "axes[0, 0].set_title('Total Loss over Training')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Policy Gradient Loss\n",
    "axes[0, 1].plot(training_metrics[\"episode\"], training_metrics[\"pg_loss\"], 'g-', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('PG Loss')\n",
    "axes[0, 1].set_title('Policy Gradient Loss')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# KL Divergence\n",
    "axes[1, 0].plot(training_metrics[\"episode\"], training_metrics[\"kl_div\"], 'r-', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('KL Divergence')\n",
    "axes[1, 0].set_title('KL Divergence from Reference')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean Reward\n",
    "axes[1, 1].plot(training_metrics[\"episode\"], training_metrics[\"mean_reward\"], 'purple', alpha=0.7, label='Mean')\n",
    "axes[1, 1].fill_between(\n",
    "    training_metrics[\"episode\"],\n",
    "    np.array(training_metrics[\"mean_reward\"]) - np.array(training_metrics[\"reward_std\"]),\n",
    "    np.array(training_metrics[\"mean_reward\"]) + np.array(training_metrics[\"reward_std\"]),\n",
    "    alpha=0.3, color='purple'\n",
    ")\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Reward')\n",
    "axes[1, 1].set_title('Mean Reward ± Std')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Final Total Loss: {training_metrics['total_loss'][-1]:.4f}\")\n",
    "print(f\"Final KL Divergence: {training_metrics['kl_div'][-1]:.4f}\")\n",
    "print(f\"Final Mean Reward: {training_metrics['mean_reward'][-1]:.3f}\")\n",
    "print(f\"Average Reward (last 5 episodes): {np.mean(training_metrics['mean_reward'][-5:]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Sample Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the trained model with math problems\ntest_math_problems = [\n    \"If a box contains 12 cookies and you eat 3, how many are left?\",\n    \"John has 5 apples. Mary gives him 7 more apples. How many apples does John have now?\",\n    \"A shirt costs $15 and pants cost $25. What is the total cost?\",\n    \"If you have 20 candies and share them equally among 4 friends, how many does each friend get?\",\n]\n\n# Store test answers for reward calculation\ntest_answers = [\"9\", \"12\", \"40\", \"5\"]\nfor prob, ans in zip(test_math_problems, test_answers):\n    PROBLEM_ANSWERS[prob] = ans\n\nprint(\"Testing trained model on new math problems:\")\nprint(\"=\" * 50)\n\nfor i, problem in enumerate(test_math_problems, 1):\n    print(f\"\\nProblem {i}: {problem}\")\n    print(\"-\" * 40)\n    \n    # Format the prompt using GRPO's formatter\n    formatted_prompt = grpo.format_prompt(problem)\n    \n    # Tokenize prompt\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n    inputs = {k: v.to(grpo.model.device) for k, v in inputs.items()}\n    \n    # Generate completion\n    with torch.no_grad():\n        outputs = grpo.model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    \n    # Decode and extract response\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Extract just the solution part (after the formatted prompt)\n    if \"Solution:\" in generated:\n        response = generated.split(\"Solution:\")[-1].strip()\n    else:\n        response = generated[len(formatted_prompt):].strip()\n    \n    print(f\"Model's solution: {response[:200]}...\")  # Show first 200 chars\n    \n    # Extract answer and check correctness\n    model_answer = extract_number(response)\n    correct_answer = test_answers[i-1]\n    \n    if model_answer:\n        print(f\"Extracted answer: {model_answer}\")\n        print(f\"Correct answer: {correct_answer}\")\n        is_correct = model_answer == correct_answer\n        print(f\"✓ CORRECT!\" if is_correct else \"✗ INCORRECT\")\n    else:\n        print(\"Could not extract numerical answer\")\n    \n    # Compute reward\n    reward = compute_math_reward(problem, response)\n    print(f\"Reward Score: {reward:.3f}\")\n\nprint(\"\\n\" + \"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model\n",
    "final_model_path = \"models/grpo_qwen_trained\"\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "\n",
    "print(f\"Saving final model to {final_model_path}...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "grpo.model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Save training config and metrics\n",
    "with open(f\"{final_model_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(grpo_config, f, indent=2)\n",
    "\n",
    "with open(f\"{final_model_path}/training_metrics.json\", \"w\") as f:\n",
    "    json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{final_model_path}')\")\n",
    "print(f\"tokenizer = AutoTokenizer.from_pretrained('{final_model_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Experiment with Different Parameters\n",
    "\n",
    "You can modify the GRPO parameters to see how they affect training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental configurations to try\n",
    "experiments = {\n",
    "    \"High KL Penalty\": {\n",
    "        \"algorithm\": {\"kl_coef\": 0.2},  # Stronger KL penalty\n",
    "    },\n",
    "    \"Large Group Size\": {\n",
    "        \"algorithm\": {\"group_size\": 8},  # More completions per prompt\n",
    "        \"training\": {\"batch_size\": 16},  # Adjust batch size accordingly\n",
    "    },\n",
    "    \"No Clipping\": {\n",
    "        \"algorithm\": {\"clip_range\": None},  # Disable PPO clipping\n",
    "    },\n",
    "    \"High Temperature\": {\n",
    "        \"training\": {\"temperature\": 1.2},  # More diverse generations\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Experimental configurations available:\")\n",
    "for name, params in experiments.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(json.dumps(params, indent=2))\n",
    "\n",
    "print(\"\\nTo use an experimental config, modify the grpo_config in cell 5 and re-run from there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated GRPO training for mathematical reasoning:\n\n### Key Features Implemented:\n1. **Math Dataset**: Used GSM8K dataset with grade school math problems\n2. **Custom Prompt Formatting**: \n   - System prompts for math tutoring context\n   - Structured problem presentation\n   - Dynamic prompt format switching\n3. **Math-Specific Reward Function**:\n   - Rewards correct numerical answers (highest weight)\n   - Rewards showing mathematical work/steps\n   - Penalizes wrong answers and poor reasoning\n4. **Training Loop**: Optimized model to solve math problems step-by-step\n5. **Evaluation**: Tested on unseen math problems with answer extraction\n\n### Prompt Customization Features:\n- `format_prompt()`: Applies system prompt, template, and prefix\n- `set_generation_prompt()`: Dynamically change formats during runtime\n- `use_formatting` parameter: Control when formatting is applied\n- Multiple format styles: Step-by-step, Chain-of-thought, Structured\n\n### Next Steps:\n- Scale to larger math datasets (full GSM8K, MATH dataset)\n- Implement more sophisticated answer extraction\n- Add curriculum learning (easy to hard problems)\n- Use a dedicated math reward model\n- Fine-tune on specific math domains (algebra, geometry, etc.)\n- Experiment with different prompt formats for better performance"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}