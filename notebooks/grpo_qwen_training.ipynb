{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Training with Qwen 0.5B for Mathematical Reasoning\n",
    "\n",
    "This notebook demonstrates training a small Qwen model using Group Relative Policy Optimization (GRPO) for improving mathematical problem-solving capabilities.\n",
    "\n",
    "## Features:\n",
    "- Downloads and loads Qwen 0.5B model\n",
    "- Uses GSM8K math dataset for training\n",
    "- Custom prompt formatting for math problems\n",
    "- Math-specific reward function\n",
    "- Tracks training metrics\n",
    "- Saves checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import GRPO from simple_rl\n",
    "from simple_rl.algorithms.grpo import GRPO\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Qwen 0.5B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-0.5B\n",
      "This may take a few minutes on first run...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af451cdcd2cb491b9f84baf41c024bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17438e4bbdb44df4a4255e3511f47b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5645e06e90914a12ad4e10cd24b3d603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97356e504285484fa7cc5a6095d06cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce13d928bfe44539a1699445f2ec19d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b834087354405bae68c11e8b29d6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be7564b92eb48a1b0f21b960046d169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model parameters: 494.0M\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"  # Small 0.5B parameter model\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GSM8K math dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3050646fcb064f53b661335698046e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6073fe691e14b8f8684390c040aae00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2012e168da9e476487b258f0d4740a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbdd2d35b3e4adf9bbcc063eca065b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29392ff810174cbfb7f43d6d121501e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 math problems\n",
      "\n",
      "Example problems:\n",
      "\n",
      "1. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and...\n",
      "   Answer: 72\n",
      "\n",
      "2. Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?...\n",
      "   Answer: 10\n",
      "\n",
      "3. Question: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purp...\n",
      "   Answer: 5\n"
     ]
    }
   ],
   "source": [
    "# Load GSM8K math dataset\n",
    "print(\"Loading GSM8K math dataset...\")\n",
    "\n",
    "# Load GSM8K - a dataset of grade school math problems\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train[:200]\")  # Use 200 examples for demo\n",
    "\n",
    "# Extract math problems and prepare prompts\n",
    "math_prompts = []\n",
    "math_answers = []  # Store answers for reward calculation\n",
    "\n",
    "for item in dataset:\n",
    "    # GSM8K format: question and answer with step-by-step solution\n",
    "    question = item['question']\n",
    "    answer = item['answer']\n",
    "    \n",
    "    # Store just the question as prompt (without formatting yet)\n",
    "    math_prompts.append(question)\n",
    "    \n",
    "    # Extract the final numerical answer (after ####)\n",
    "    final_answer = answer.split(\"####\")[-1].strip() if \"####\" in answer else answer\n",
    "    math_answers.append(final_answer)\n",
    "\n",
    "print(f\"Loaded {len(math_prompts)} math problems\")\n",
    "print(\"\\nExample problems:\")\n",
    "for i in range(min(3, len(math_prompts))):\n",
    "    print(f\"\\n{i+1}. Question: {math_prompts[i][:150]}...\")\n",
    "    print(f\"   Answer: {math_answers[i]}\")\n",
    "\n",
    "# Store answers for later use in reward function\n",
    "PROBLEM_ANSWERS = dict(zip(math_prompts, math_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure GRPO Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO Configuration with math-specific prompt formatting\n",
    "grpo_config = {\n",
    "    \"algorithm\": {\n",
    "        \"name\": \"grpo\",\n",
    "        \"group_size\": 4,  # Number of completions per prompt\n",
    "        \"kl_coef\": 0.05,  # KL divergence coefficient\n",
    "        \"clip_range\": 0.2,  # PPO-style clipping\n",
    "        \"normalize_rewards\": True,  # Normalize rewards per group\n",
    "        \"update_epochs\": 2,  # PPO update epochs\n",
    "        \"minibatch_size\": 4,  # Minibatch size for updates\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 8,  # Total batch size (must be divisible by group_size)\n",
    "        \"learning_rate\": 1e-5,  # Learning rate\n",
    "        \"gradient_clip\": 1.0,  # Gradient clipping\n",
    "        \"max_new_tokens\": 150,  # More tokens for math reasoning\n",
    "        \"temperature\": 0.7,  # Generation temperature\n",
    "        \"num_episodes\": 30,  # More episodes for math training\n",
    "    },\n",
    "    \"generation\": {\n",
    "        # Math-specific prompt formatting\n",
    "        \"system_prompt\": \"You are a helpful math tutor. Solve the problem step by step, showing your work clearly.\",\n",
    "        \"prompt_template\": \"Problem: {prompt}\\n\\nSolution: Let me solve this step by step.\\n\",\n",
    "        \"response_prefix\": \"\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"max_length\": 512,  # Maximum sequence length\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"log_interval\": 2,  # Log every 2 episodes\n",
    "        \"save_interval\": 10,  # Save checkpoint every 10 episodes\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"enabled\": False,  # Disable W&B for notebook demo\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create config object\n",
    "config = Config(**grpo_config)\n",
    "\n",
    "print(\"GRPO Configuration for Math Training:\")\n",
    "print(json.dumps(grpo_config, indent=2))\n",
    "\n",
    "# Show example of formatted prompt\n",
    "example_prompt = math_prompts[0] if math_prompts else \"What is 2 + 2?\"\n",
    "print(f\"\\nExample formatted prompt:\")\n",
    "print(\"-\" * 50)\n",
    "formatted_example = grpo_config[\"generation\"][\"system_prompt\"] + \"\\n\\n\" + \\\n",
    "                   grpo_config[\"generation\"][\"prompt_template\"].replace(\"{prompt}\", example_prompt)\n",
    "print(formatted_example)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_number(text):\n",
    "    \"\"\"Extract the final numerical answer from text.\"\"\"\n",
    "    # Look for patterns like \"answer is X\", \"equals X\", \"= X\", etc.\n",
    "    patterns = [\n",
    "        r\"answer is:?\\s*([-]?\\d+\\.?\\d*)\",\n",
    "        r\"equals?\\s*([-]?\\d+\\.?\\d*)\",\n",
    "        r\"=\\s*([-]?\\d+\\.?\\d*)\",\n",
    "        r\"total of\\s*([-]?\\d+\\.?\\d*)\",\n",
    "        r\"result is:?\\s*([-]?\\d+\\.?\\d*)\",\n",
    "        r\"Therefore,?\\s*([-]?\\d+\\.?\\d*)\",\n",
    "        r\"So,?\\s*([-]?\\d+\\.?\\d*)\",\n",
    "        r\"\\$?([-]?\\d+\\.?\\d*)\\s*(?:dollars?|cents?|items?|students?|people|apples?|cookies?)?\\.?\\s*$\"\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    # If no pattern matches, look for any number at the end\n",
    "    numbers = re.findall(r\"[-]?\\d+\\.?\\d*\", text)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    return None\n",
    "\n",
    "def compute_math_reward(prompt: str, completion: str) -> float:\n",
    "    \"\"\"\n",
    "    Reward function for mathematical problem solving.\n",
    "    \n",
    "    Rewards based on:\n",
    "    - Correct final answer (most important)\n",
    "    - Showing work/steps\n",
    "    - Mathematical reasoning indicators\n",
    "    - Proper formatting\n",
    "    \"\"\"\n",
    "    reward = 0.0\n",
    "    \n",
    "    # Check if we have the correct answer for this problem\n",
    "    correct_answer = PROBLEM_ANSWERS.get(prompt, None)\n",
    "    \n",
    "    if correct_answer:\n",
    "        # Extract the model's answer\n",
    "        model_answer = extract_number(completion)\n",
    "        \n",
    "        if model_answer:\n",
    "            try:\n",
    "                # Check if the answer is correct (allowing for small float differences)\n",
    "                correct_float = float(correct_answer)\n",
    "                model_float = float(model_answer)\n",
    "                \n",
    "                if abs(correct_float - model_float) < 0.01:\n",
    "                    reward += 2.0  # Big reward for correct answer\n",
    "                else:\n",
    "                    # Partial credit for being close\n",
    "                    if abs(correct_float - model_float) / max(abs(correct_float), 1) < 0.1:\n",
    "                        reward += 0.5\n",
    "                    else:\n",
    "                        reward -= 0.5  # Penalty for wrong answer\n",
    "            except ValueError:\n",
    "                # If conversion fails, do string comparison\n",
    "                if model_answer == correct_answer:\n",
    "                    reward += 2.0\n",
    "                else:\n",
    "                    reward -= 0.3\n",
    "        else:\n",
    "            # No answer found\n",
    "            reward -= 1.0\n",
    "    \n",
    "    # Reward for showing mathematical work\n",
    "    math_indicators = [\n",
    "        \"step\", \"first\", \"then\", \"next\", \"finally\",\n",
    "        \"calculate\", \"multiply\", \"divide\", \"add\", \"subtract\",\n",
    "        \"=\", \"+\", \"-\", \"*\", \"/\", \"×\", \"÷\"\n",
    "    ]\n",
    "    \n",
    "    work_shown = sum(1 for indicator in math_indicators if indicator in completion.lower())\n",
    "    if work_shown >= 3:\n",
    "        reward += 0.5  # Reward for showing work\n",
    "    elif work_shown == 0:\n",
    "        reward -= 0.3  # Penalty for no work shown\n",
    "    \n",
    "    # Reward for reasonable length (not too short, not too long)\n",
    "    completion_length = len(completion.split())\n",
    "    if 20 <= completion_length <= 200:\n",
    "        reward += 0.2\n",
    "    elif completion_length < 10:\n",
    "        reward -= 0.5  # Too short\n",
    "    elif completion_length > 300:\n",
    "        reward -= 0.2  # Too verbose\n",
    "    \n",
    "    # Penalty for repetition\n",
    "    sentences = completion.split('.')\n",
    "    if len(sentences) > 1:\n",
    "        unique_sentences = len(set(sentences))\n",
    "        if unique_sentences / len(sentences) < 0.7:\n",
    "            reward -= 0.5  # Repetitive\n",
    "    \n",
    "    # Penalty for obvious errors or nonsense\n",
    "    if \"error\" in completion.lower() or \"sorry\" in completion.lower():\n",
    "        reward -= 0.5\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Test the reward function\n",
    "test_cases = [\n",
    "    (\"What is 5 + 3?\", \"5 + 3 = 8. The answer is 8.\"),\n",
    "    (\"What is 5 + 3?\", \"Let me calculate: 5 + 3 equals 7.\"),\n",
    "    (\"What is 5 + 3?\", \"8\"),\n",
    "    (\"What is 5 + 3?\", \"First, I'll add 5 and 3. 5 + 3 = 8. Therefore, the answer is 8.\"),\n",
    "]\n",
    "\n",
    "# Set up test answer\n",
    "PROBLEM_ANSWERS[\"What is 5 + 3?\"] = \"8\"\n",
    "\n",
    "print(\"Testing math reward function:\")\n",
    "for prompt, completion in test_cases:\n",
    "    reward = compute_math_reward(prompt, completion)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Completion: {completion}\")\n",
    "    print(f\"Reward: {reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize GRPO with Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRPO with the loaded model and math reward function\n",
    "print(\"Initializing GRPO with math-specific configuration...\")\n",
    "\n",
    "# Create GRPO instance with math reward\n",
    "grpo = GRPO(\n",
    "    model=model,\n",
    "    config=config.to_dict(),\n",
    "    tokenizer=tokenizer,\n",
    "    reward_fn=compute_math_reward,  # Using our math-specific reward function\n",
    "    use_wandb=False\n",
    ")\n",
    "\n",
    "print(\"GRPO initialized successfully!\")\n",
    "print(f\"Policy model parameters: {sum(p.numel() for p in grpo.model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"Reference model parameters: {sum(p.numel() for p in grpo.reference_model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "# Show how prompts will be formatted\n",
    "print(\"\\nPrompt formatting example:\")\n",
    "sample_problem = \"If John has 5 apples and Mary gives him 3 more, how many apples does John have?\"\n",
    "formatted = grpo.format_prompt(sample_problem)\n",
    "print(\"Original:\", sample_problem)\n",
    "print(\"Formatted:\", formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Experiment with Different Math Prompt Formats\n",
    "\n",
    "You can dynamically change how math problems are presented to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different prompt formats for math problems\n",
    "test_problem = \"A store sells pencils for $0.50 each. If Sarah buys 8 pencils, how much does she pay?\"\n",
    "\n",
    "print(\"Testing different prompt formats for math problems:\\n\")\n",
    "\n",
    "# Format 1: Step-by-step instruction\n",
    "print(\"1. Step-by-step format:\")\n",
    "grpo.set_generation_prompt(\n",
    "    system_prompt=\"Solve this math problem step by step. Show all your work.\",\n",
    "    prompt_template=\"Question: {prompt}\\n\\nStep-by-step solution:\",\n",
    "    response_prefix=\"\\n\"\n",
    ")\n",
    "print(grpo.format_prompt(test_problem))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Format 2: Chain-of-thought prompting\n",
    "print(\"2. Chain-of-thought format:\")\n",
    "grpo.set_generation_prompt(\n",
    "    system_prompt=\"Think through this problem carefully.\",\n",
    "    prompt_template=\"{prompt}\\n\\nLet's think step by step:\",\n",
    "    response_prefix=\" \"\n",
    ")\n",
    "print(grpo.format_prompt(test_problem))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Format 3: Structured math format\n",
    "print(\"3. Structured format:\")\n",
    "grpo.set_generation_prompt(\n",
    "    system_prompt=None,  # No system prompt\n",
    "    prompt_template=\"Math Problem:\\n{prompt}\\n\\nGiven information:\\n- \\n\\nCalculation:\\n\",\n",
    "    response_prefix=\"\"\n",
    ")\n",
    "print(grpo.format_prompt(test_problem))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Reset to original configuration for training\n",
    "print(\"Resetting to training configuration...\")\n",
    "grpo.set_generation_prompt(\n",
    "    system_prompt=grpo_config[\"generation\"][\"system_prompt\"],\n",
    "    prompt_template=grpo_config[\"generation\"][\"prompt_template\"],\n",
    "    response_prefix=grpo_config[\"generation\"][\"response_prefix\"]\n",
    ")\n",
    "print(\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop with Metrics Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics storage\n",
    "training_metrics = {\n",
    "    \"episode\": [],\n",
    "    \"total_loss\": [],\n",
    "    \"pg_loss\": [],\n",
    "    \"kl_div\": [],\n",
    "    \"mean_reward\": [],\n",
    "    \"reward_std\": [],\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = Path(\"checkpoints/grpo_qwen_math\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Starting GRPO training for {grpo_config['training']['num_episodes']} episodes...\")\n",
    "print(\"Training on math problems from GSM8K dataset\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(grpo_config['training']['num_episodes']):\n",
    "    # Sample batch of math problems\n",
    "    batch_size = grpo_config['training']['batch_size'] // grpo_config['algorithm']['group_size']\n",
    "    batch_indices = np.random.choice(len(math_prompts), batch_size, replace=True)\n",
    "    batch_prompts = [math_prompts[i] for i in batch_indices]\n",
    "    \n",
    "    # Prepare batch data (prompts will be formatted internally by GRPO)\n",
    "    batch_data = {\"prompts\": batch_prompts}\n",
    "    \n",
    "    # Training step\n",
    "    metrics = grpo.train_step(batch_data)\n",
    "    \n",
    "    # Store metrics\n",
    "    training_metrics[\"episode\"].append(episode)\n",
    "    training_metrics[\"total_loss\"].append(metrics[\"total_loss\"])\n",
    "    training_metrics[\"pg_loss\"].append(metrics[\"pg_loss\"])\n",
    "    training_metrics[\"kl_div\"].append(metrics[\"kl_div\"])\n",
    "    training_metrics[\"mean_reward\"].append(metrics[\"mean_reward\"])\n",
    "    training_metrics[\"reward_std\"].append(metrics.get(\"reward_std\", 0.0))\n",
    "    \n",
    "    # Logging\n",
    "    if episode % grpo_config['logging']['log_interval'] == 0:\n",
    "        print(f\"Episode {episode:3d} | \"\n",
    "              f\"Loss: {metrics['total_loss']:7.4f} | \"\n",
    "              f\"PG Loss: {metrics['pg_loss']:7.4f} | \"\n",
    "              f\"KL: {metrics['kl_div']:7.4f} | \"\n",
    "              f\"Reward: {metrics['mean_reward']:6.3f} ± {metrics.get('reward_std', 0.0):5.3f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (episode + 1) % grpo_config['logging']['save_interval'] == 0:\n",
    "        checkpoint_path = checkpoint_dir / f\"checkpoint_episode_{episode+1}.pt\"\n",
    "        grpo.save_checkpoint(str(checkpoint_path))\n",
    "        print(f\"  → Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('GRPO Training Metrics', fontsize=16)\n",
    "\n",
    "# Total Loss\n",
    "axes[0, 0].plot(training_metrics[\"episode\"], training_metrics[\"total_loss\"], 'b-', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Loss')\n",
    "axes[0, 0].set_title('Total Loss over Training')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Policy Gradient Loss\n",
    "axes[0, 1].plot(training_metrics[\"episode\"], training_metrics[\"pg_loss\"], 'g-', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('PG Loss')\n",
    "axes[0, 1].set_title('Policy Gradient Loss')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# KL Divergence\n",
    "axes[1, 0].plot(training_metrics[\"episode\"], training_metrics[\"kl_div\"], 'r-', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('KL Divergence')\n",
    "axes[1, 0].set_title('KL Divergence from Reference')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean Reward\n",
    "axes[1, 1].plot(training_metrics[\"episode\"], training_metrics[\"mean_reward\"], 'purple', alpha=0.7, label='Mean')\n",
    "axes[1, 1].fill_between(\n",
    "    training_metrics[\"episode\"],\n",
    "    np.array(training_metrics[\"mean_reward\"]) - np.array(training_metrics[\"reward_std\"]),\n",
    "    np.array(training_metrics[\"mean_reward\"]) + np.array(training_metrics[\"reward_std\"]),\n",
    "    alpha=0.3, color='purple'\n",
    ")\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Reward')\n",
    "axes[1, 1].set_title('Mean Reward ± Std')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Final Total Loss: {training_metrics['total_loss'][-1]:.4f}\")\n",
    "print(f\"Final KL Divergence: {training_metrics['kl_div'][-1]:.4f}\")\n",
    "print(f\"Final Mean Reward: {training_metrics['mean_reward'][-1]:.3f}\")\n",
    "print(f\"Average Reward (last 5 episodes): {np.mean(training_metrics['mean_reward'][-5:]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Sample Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model with math problems\n",
    "test_math_problems = [\n",
    "    \"If a box contains 12 cookies and you eat 3, how many are left?\",\n",
    "    \"John has 5 apples. Mary gives him 7 more apples. How many apples does John have now?\",\n",
    "    \"A shirt costs $15 and pants cost $25. What is the total cost?\",\n",
    "    \"If you have 20 candies and share them equally among 4 friends, how many does each friend get?\",\n",
    "]\n",
    "\n",
    "# Store test answers for reward calculation\n",
    "test_answers = [\"9\", \"12\", \"40\", \"5\"]\n",
    "for prob, ans in zip(test_math_problems, test_answers):\n",
    "    PROBLEM_ANSWERS[prob] = ans\n",
    "\n",
    "print(\"Testing trained model on new math problems:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, problem in enumerate(test_math_problems, 1):\n",
    "    print(f\"\\nProblem {i}: {problem}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Format the prompt using GRPO's formatter\n",
    "    formatted_prompt = grpo.format_prompt(problem)\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    inputs = {k: v.to(grpo.model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate completion\n",
    "    with torch.no_grad():\n",
    "        outputs = grpo.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and extract response\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the solution part (after the formatted prompt)\n",
    "    if \"Solution:\" in generated:\n",
    "        response = generated.split(\"Solution:\")[-1].strip()\n",
    "    else:\n",
    "        response = generated[len(formatted_prompt):].strip()\n",
    "    \n",
    "    print(f\"Model's solution: {response[:200]}...\")  # Show first 200 chars\n",
    "    \n",
    "    # Extract answer and check correctness\n",
    "    model_answer = extract_number(response)\n",
    "    correct_answer = test_answers[i-1]\n",
    "    \n",
    "    if model_answer:\n",
    "        print(f\"Extracted answer: {model_answer}\")\n",
    "        print(f\"Correct answer: {correct_answer}\")\n",
    "        is_correct = model_answer == correct_answer\n",
    "        print(f\"✓ CORRECT!\" if is_correct else \"✗ INCORRECT\")\n",
    "    else:\n",
    "        print(\"Could not extract numerical answer\")\n",
    "    \n",
    "    # Compute reward\n",
    "    reward = compute_math_reward(problem, response)\n",
    "    print(f\"Reward Score: {reward:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model\n",
    "final_model_path = \"models/grpo_qwen_trained\"\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "\n",
    "print(f\"Saving final model to {final_model_path}...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "grpo.model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Save training config and metrics\n",
    "with open(f\"{final_model_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(grpo_config, f, indent=2)\n",
    "\n",
    "with open(f\"{final_model_path}/training_metrics.json\", \"w\") as f:\n",
    "    json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{final_model_path}')\")\n",
    "print(f\"tokenizer = AutoTokenizer.from_pretrained('{final_model_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Experiment with Different Parameters\n",
    "\n",
    "You can modify the GRPO parameters to see how they affect training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Additional Models or Packages\n",
    "\n",
    "If you need to install additional packages or models, use `uv`:\n",
    "\n",
    "```bash\n",
    "# Install a specific model/tokenizer\n",
    "!uv pip install sentencepiece  # For some tokenizers\n",
    "\n",
    "# Install a different torch version (e.g., with CUDA)\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# List installed packages\n",
    "!uv pip list | grep transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental configurations to try\n",
    "experiments = {\n",
    "    \"High KL Penalty\": {\n",
    "        \"algorithm\": {\"kl_coef\": 0.2},  # Stronger KL penalty\n",
    "    },\n",
    "    \"Large Group Size\": {\n",
    "        \"algorithm\": {\"group_size\": 8},  # More completions per prompt\n",
    "        \"training\": {\"batch_size\": 16},  # Adjust batch size accordingly\n",
    "    },\n",
    "    \"No Clipping\": {\n",
    "        \"algorithm\": {\"clip_range\": None},  # Disable PPO clipping\n",
    "    },\n",
    "    \"High Temperature\": {\n",
    "        \"training\": {\"temperature\": 1.2},  # More diverse generations\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Experimental configurations available:\")\n",
    "for name, params in experiments.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(json.dumps(params, indent=2))\n",
    "\n",
    "print(\"\\nTo use an experimental config, modify the grpo_config in cell 5 and re-run from there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated GRPO training for mathematical reasoning:\n",
    "\n",
    "### Key Features Implemented:\n",
    "1. **Math Dataset**: Used GSM8K dataset with grade school math problems\n",
    "2. **Custom Prompt Formatting**: \n",
    "   - System prompts for math tutoring context\n",
    "   - Structured problem presentation\n",
    "   - Dynamic prompt format switching\n",
    "3. **Math-Specific Reward Function**:\n",
    "   - Rewards correct numerical answers (highest weight)\n",
    "   - Rewards showing mathematical work/steps\n",
    "   - Penalizes wrong answers and poor reasoning\n",
    "4. **Training Loop**: Optimized model to solve math problems step-by-step\n",
    "5. **Evaluation**: Tested on unseen math problems with answer extraction\n",
    "\n",
    "### Prompt Customization Features:\n",
    "- `format_prompt()`: Applies system prompt, template, and prefix\n",
    "- `set_generation_prompt()`: Dynamically change formats during runtime\n",
    "- `use_formatting` parameter: Control when formatting is applied\n",
    "- Multiple format styles: Step-by-step, Chain-of-thought, Structured\n",
    "\n",
    "### Next Steps:\n",
    "- Scale to larger math datasets (full GSM8K, MATH dataset)\n",
    "- Implement more sophisticated answer extraction\n",
    "- Add curriculum learning (easy to hard problems)\n",
    "- Use a dedicated math reward model\n",
    "- Fine-tune on specific math domains (algebra, geometry, etc.)\n",
    "- Experiment with different prompt formats for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
